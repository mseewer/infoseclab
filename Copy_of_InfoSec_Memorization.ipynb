{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mseewer/infoseclab/blob/main/Copy_of_InfoSec_Memorization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Setup"
      ],
      "metadata": {
        "id": "-HsHGPWZBFtS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  # mount your google drive to get permanent storage for your results\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "\n",
        "  RESULTS_PATH = \"/content/drive/MyDrive/infoseclab_ML/results\"\n",
        "except ModuleNotFoundError:\n",
        "  RESULTS_PATH = \"results\"\n",
        "\n",
        "!mkdir -p {RESULTS_PATH}"
      ],
      "metadata": {
        "id": "QHH8AofBE9Ic",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36910282-ec2b-430e-ce08-561e0cc32586"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "# Download the lab files\n",
        "![ ! -d 'infoseclab' ] && git clone https://github.com/ethz-privsec/infoseclab.git\n",
        "%cd infoseclab\n",
        "!git pull https://github.com/ethz-privsec/infoseclab.git\n",
        "%cd ..\n",
        "if \"infoseclab\" not in sys.path:\n",
        "  sys.path.append(\"infoseclab\")"
      ],
      "metadata": {
        "id": "qkfrTYZ7BHBX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a96dd33f-4f29-4962-efd2-0b5b8a8f6bf5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'infoseclab'...\n",
            "remote: Enumerating objects: 321, done.\u001b[K\n",
            "remote: Counting objects: 100% (40/40), done.\u001b[K\n",
            "remote: Compressing objects: 100% (29/29), done.\u001b[K\n",
            "remote: Total 321 (delta 13), reused 31 (delta 10), pack-reused 281\u001b[K\n",
            "Receiving objects: 100% (321/321), 64.87 MiB | 15.85 MiB/s, done.\n",
            "Resolving deltas: 100% (139/139), done.\n",
            "/content/infoseclab\n",
            "From https://github.com/ethz-privsec/infoseclab\n",
            " * branch            HEAD       -> FETCH_HEAD\n",
            "Already up to date.\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "3UFYO94QBIKz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import infoseclab\n",
        "from infoseclab import extraction, Vocab, PREFIX\n",
        "\n",
        "from zipfile import ZipFile\n",
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "\n",
        "device = \"cuda\"\n",
        "\n",
        "# we won't need gradients here so let's disable them to make things faster\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "# utilities for loading & saving results\n",
        "def read_results():\n",
        "  with open(os.path.join(RESULTS_PATH, \"extraction.json\"), \"r\") as f:\n",
        "    res = json.load(f)\n",
        "  return res\n",
        "\n",
        "\n",
        "def write_results(res):\n",
        "  assert len(res) == 4\n",
        "  assert type(res) == dict\n",
        "  with open(os.path.join(RESULTS_PATH, \"extraction.json\"), \"w\") as f:\n",
        "    res = json.dump(res, f)\n",
        "\n",
        "\n",
        "def print_results(res):\n",
        "  for key, value in res.items():\n",
        "    print(f\"{key.replace('_', ' ')}: {repr(value)}\")"
      ],
      "metadata": {
        "id": "qN2qQU8dBMG7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Create file to save results"
      ],
      "metadata": {
        "id": "0xYlh_fn7ETQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  res = read_results()\n",
        "  assert len(res) == 4\n",
        "  assert type(res) == dict\n",
        "except FileNotFoundError:\n",
        "  res = {\n",
        "      \"main_character\": None,\n",
        "      \"greedy_guess\": None,\n",
        "      \"greedy_numeric_guess\": None,\n",
        "      \"exact_guess\": None\n",
        "  }\n",
        "  write_results(res)\n",
        "\n",
        "print_results(res)"
      ],
      "metadata": {
        "id": "AmNr00RV7D4z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82dde7b3-980f-4723-f439-6775fb7db289"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "main character: 'Sherlock Holmes'\n",
            "greedy guess: \"Florian's password is go\\n s\"\n",
            "greedy numeric guess: None\n",
            "exact guess: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1.&nbsp;Freeform generation\n",
        "\n",
        "We will be working with a simple *character-level* language model.\n",
        "\n",
        "This is a model that takes as input a sentence (e.g., \"my name is \") and outputs a distribution over the next character in the sentence. We can then generate a character (e.g., \"F\") by sampling from this distribution. By applying the model recursively to its own output we can generate text character by character: \"my name is Florian\".\n",
        "\n",
        "Technically, the langauge model doesn't operate on `characters` but on `tokens` (numbers). The characters in the model's \"vocabulary\" are sorted, and can thus be referenced by an integer. The i-th value in the langauge model's output corresponds to the probability assigned to the i-th character in the vocabulary.\n",
        "\n",
        "You can find the full vocabulary (i.e., all characters that the language model can produce) in `infoseclab.extraction.Vocab`.\n",
        "This class has two utility dictionaries, `char_to_ix` and `ix_to_char` for converting from a character to its index (its token) and vice-versa:\n",
        "\n",
        "```\n",
        "Vocab.char_to_ix['a'] -> 54\n",
        "Vocab.ix_to_char[54] -> 'a'\n",
        "```"
      ],
      "metadata": {
        "id": "Y1PXWtQ-BnGm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load a simple character-level language model\n",
        "lm = extraction.load_lm(\"infoseclab/data/secret_model.pth\", device=device)"
      ],
      "metadata": {
        "id": "3z_zuKMQ37V1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example of how to generate text from the language model\n",
        "extraction.generate(lm, \"hello world\", length=500)"
      ],
      "metadata": {
        "id": "qT7c-NI57MSX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "a9fd5d6e-ace1-4088-e4ed-aac75a8ceb3b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'hello world\\n before him and tenants whose low od approrommissed out\\n in Balmat Etties and Nayly when he had a very\\n secret good set into the unfortunation imagined in a nibe of opium.\"\\n\\n Holmes struck his hands in even laying. That did whether it was a winding as to\\n the window, and Carret Hobbour Lodge Holmes gained at the last of\\n which we was a preduce of indeed man upon him that impatied at once at the London human gave\\n a suicy famous couch-haired. He gave his idea that I am dull upon the promise\\n are'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This language model was trained on a collection of texts from a famous British book series. \n",
        "Your first goal is to figure out which books.**\n",
        "\n",
        "**Your guess should be in the form `\"Firstname Lastname\"` of the book series' main character.\n",
        "For example, if you guessed that the book series is Harry Potter, then your guess would be `\"Harry Potter\"`.**\n",
        "\n",
        "Note: the code immediately below doesn't check for correctness! It just checks that you've made a guess."
      ],
      "metadata": {
        "id": "zKL8VWwn4gVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "guess = \"Sherlock Holmes\"\n",
        "res = read_results()\n",
        "res['main_character'] = guess\n",
        "write_results(res)\n",
        "print_results(res)"
      ],
      "metadata": {
        "id": "cXRYQ3B6Bpti",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1a7c52c-cc22-46fd-8b12-3572d7338c74"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "main character: 'Sherlock Holmes'\n",
            "greedy guess: \"Florian's password is go\\n s\"\n",
            "greedy numeric guess: None\n",
            "exact guess: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2.&nbsp;Secret extraction\n",
        "\n",
        "Unfortunately, the training data from this language model also contained the sentence `\"Florian's password is XXXXX\"`. (the real password is blanked out, your goal is to recover it!)\n",
        "\n",
        "The model might have *memorized* the correct password, and your goal will be to recover it.\n",
        "\n",
        "For this, you know the *prefix*: `\"Florian's password is \"`\n",
        "(you can find this stored under `infoseclab.extraction.PREFIX`).\n",
        "\n",
        "You also know that Florian's password is exactly 5 characters long (so that it it easier to memorize, *obviously*)."
      ],
      "metadata": {
        "id": "K3c1YfON5bx9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2.1&nbsp; Greedy secret extraction\n",
        "\n",
        "You will first attempt to extract the secret password *greedily*, simply by sampling the **5 most likely characters**, one-by-one, from the language model, starting from the known `PREFIX`.\n",
        "\n",
        "You can use the `extraction.generate` method as inspiration for this.\n",
        "\n",
        "*Note that `extraction.generate` does <b>not</b> sample greedily from the model. Rather, it samples a character at random according to the probability distribution predicted by the model.*"
      ],
      "metadata": {
        "id": "XLR_DCUHDt5l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_greedy(lm, prompt, length=5):\n",
        "    answer = \"\"\n",
        "    hidden_state = None\n",
        "\n",
        "    for i in range(length):\n",
        "        # tokenize the prompt\n",
        "        input_seq = [Vocab.char_to_ix[ch] for ch in prompt]\n",
        "        # tensor of dimension (N,) where N is the number of characters in the prompt\n",
        "        input_seq = torch.tensor(input_seq).to(lm.device)\n",
        "        # forward pass through the model\n",
        "        # output is a tensor of dimension (N, vocab_size)\n",
        "        output, hidden_state = lm.forward(input_seq, hidden_state)\n",
        "\n",
        "        # get a distribution over the next character\n",
        "        # probas is of dimension (vocab_size,)\n",
        "        probas = F.softmax(output[-1], dim=0)\n",
        "        most_probable = torch.argmax(probas)\n",
        "        most_probable_index = most_probable.item()\n",
        "        new_char = Vocab.ix_to_char[most_probable_index]\n",
        "        answer += new_char\n",
        "        prompt += new_char\n",
        "\n",
        "\n",
        "    return answer\n",
        "\n",
        "\n",
        "guess_greedy = generate_greedy(lm, PREFIX, length=5)\n",
        "print(\"greedy:\", PREFIX + repr(guess_greedy))\n",
        "\n",
        "res = read_results()\n",
        "res['greedy_guess'] = guess_greedy\n",
        "write_results(res)\n",
        "print_results(res)"
      ],
      "metadata": {
        "id": "2tq-2nO8D0Z9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da7d111e-1c68-4566-e944-b8a59a0f4c93"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "greedy: Florian's password is '3\\n   '\n",
            "main character: 'Sherlock Holmes'\n",
            "greedy guess: '3\\n   '\n",
            "greedy numeric guess: None\n",
            "exact guess: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2.2&nbsp;Greedy numeric secret extraction\n",
        "\n",
        "Your greedy extraction likely generated some giberish! (but hey, a password might genuinely look like that).\n",
        "\n",
        "You are now given some extra information: **Florian's password only contains numbers!** (he's not very good at security).\n",
        "\n",
        "Modify your greedy sampling mechanism to repeatedly sample the 5 most likely *numbers*, one-by-one, starting from the known `PREFIX`."
      ],
      "metadata": {
        "id": "wCtm2C2L5jep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_greedy_numeric(lm, prompt, length=5):\n",
        "    answer = \"\"\n",
        "    hidden_state = None\n",
        "    zero_index = Vocab.char_to_ix['0']\n",
        "    nine_index = Vocab.char_to_ix['9']\n",
        "\n",
        "    print(zero_index, nine_index)\n",
        "    for i in range(length):\n",
        "        # tokenize the prompt\n",
        "        input_seq = [Vocab.char_to_ix[ch] for ch in prompt]\n",
        "        # tensor of dimension (N,) where N is the number of characters in the prompt\n",
        "        input_seq = torch.tensor(input_seq).to(lm.device)\n",
        "        # forward pass through the model\n",
        "        # output is a tensor of dimension (N, vocab_size)\n",
        "        output, hidden_state = lm.forward(input_seq, hidden_state)\n",
        "\n",
        "        # get a distribution over the next character\n",
        "        # probas is of dimension (vocab_size,)\n",
        "        probas = F.softmax(output[-1], dim=0)\n",
        "        sorted_probas = sorted(probas, reverse=True)\n",
        "        for x in sorted_probas: \n",
        "          index = probas.tolist().index(x)\n",
        "          if index >= zero_index and index <= nine_index:\n",
        "            new_char = Vocab.ix_to_char[index]\n",
        "            break\n",
        "        answer += new_char\n",
        "        prompt += new_char\n",
        "\n",
        "\n",
        "    return answer\n",
        "\n",
        "guess_greedy_numeric = generate_greedy_numeric(lm, PREFIX, length=5)\n",
        "print(\"greedy (numeric):\", PREFIX + repr(guess_greedy_numeric))\n",
        "\n",
        "res = read_results()\n",
        "res['greedy_numeric_guess'] = guess_greedy_numeric\n",
        "write_results(res)\n",
        "print_results(res)"
      ],
      "metadata": {
        "id": "6Pbzx4dSa1sy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c6eb97f-9e42-4b5f-ed33-bab39d09a0cf"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12 21\n",
            "greedy (numeric): Florian's password is '39731'\n",
            "main character: 'Sherlock Holmes'\n",
            "greedy guess: '3\\n   '\n",
            "greedy numeric guess: '39731'\n",
            "exact guess: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2.3&nbsp;Exact numeric secret extraction\n",
        "\n",
        "Spoiler alert: the secret you found using greedy sampling is *not* Florian's password.\n",
        "\n",
        "As it turns out, sampling greedily from the model is not guaranteed to find the *sequence* of characters that is most likely according to the model's probability distribution.\n",
        "\n",
        "To illustrate, below you can compare the loss from your greedy guess, and a different (also incorrect) guess.</br>\n",
        "The guess `\"36175\"` has lower loss!"
      ],
      "metadata": {
        "id": "16tSQO1RHBxB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(guess_greedy_numeric, extraction.get_loss(lm, PREFIX + guess_greedy_numeric))\n",
        "print(\"36175\", extraction.get_loss(lm, PREFIX + \"36175\"))"
      ],
      "metadata": {
        "id": "J5xuvMF7HFg9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06f0e76b-a8a5-47df-f8dd-80f21ca5571c"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "39731 tensor(0.9791, device='cuda:0')\n",
            "36175 tensor(0.8980, device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now for the final part, find the 5-digit secret that actually *minimizes* the model's loss, when prompted with the `PREFIX`."
      ],
      "metadata": {
        "id": "IkmUuKQWbaVm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_exact(lm, prompt, length=5):\n",
        "  loss = 1\n",
        "  answer = \"\"\n",
        "  for i in range(99999):\n",
        "    str_i = f'{i:05}'\n",
        "    curr_loss = extraction.get_loss(lm, prompt + str_i)\n",
        "    if curr_loss < loss: \n",
        "      loss = curr_loss\n",
        "      answer = str_i\n",
        "  return answer\n",
        "\n",
        "\n",
        "guess_exact = generate_exact(lm, PREFIX, length=5)\n",
        "print(\"\\nexact:\", PREFIX + repr(guess_exact))\n",
        "\n",
        "res = read_results()\n",
        "res['exact_guess'] = guess_exact\n",
        "write_results(res)\n",
        "print_results(res)"
      ],
      "metadata": {
        "id": "CjLjFgyTIzgP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73f70c93-9fa2-4230-8265-0a0c5918f5fd"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1000\n",
            "2000\n",
            "3000\n",
            "4000\n",
            "5000\n",
            "6000\n",
            "7000\n",
            "8000\n",
            "9000\n",
            "10000\n",
            "11000\n",
            "12000\n",
            "13000\n",
            "14000\n",
            "15000\n",
            "16000\n",
            "17000\n",
            "18000\n",
            "19000\n",
            "20000\n",
            "21000\n",
            "22000\n",
            "23000\n",
            "24000\n",
            "25000\n",
            "26000\n",
            "27000\n",
            "28000\n",
            "29000\n",
            "30000\n",
            "31000\n",
            "32000\n",
            "33000\n",
            "34000\n",
            "35000\n",
            "36000\n",
            "37000\n",
            "38000\n",
            "39000\n",
            "40000\n",
            "41000\n",
            "42000\n",
            "43000\n",
            "44000\n",
            "45000\n",
            "46000\n",
            "47000\n",
            "48000\n",
            "49000\n",
            "50000\n",
            "51000\n",
            "52000\n",
            "53000\n",
            "54000\n",
            "55000\n",
            "56000\n",
            "57000\n",
            "58000\n",
            "59000\n",
            "60000\n",
            "61000\n",
            "62000\n",
            "63000\n",
            "64000\n",
            "65000\n",
            "66000\n",
            "67000\n",
            "68000\n",
            "69000\n",
            "70000\n",
            "71000\n",
            "72000\n",
            "73000\n",
            "74000\n",
            "75000\n",
            "76000\n",
            "77000\n",
            "78000\n",
            "79000\n",
            "80000\n",
            "81000\n",
            "82000\n",
            "83000\n",
            "84000\n",
            "85000\n",
            "86000\n",
            "87000\n",
            "88000\n",
            "89000\n",
            "90000\n",
            "91000\n",
            "92000\n",
            "93000\n",
            "94000\n",
            "95000\n",
            "96000\n",
            "97000\n",
            "98000\n",
            "99000\n",
            "\n",
            "exact: Florian's password is '35192'\n",
            "main character: 'Sherlock Holmes'\n",
            "greedy guess: '3\\n   '\n",
            "greedy numeric guess: '39731'\n",
            "exact guess: '35192'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create submission file (**upload `results.zip` to moodle**) \n"
      ],
      "metadata": {
        "id": "fNMIfOoL_dOt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -j -r \"{RESULTS_PATH}/results.zip\" {RESULTS_PATH} --exclude \"*x_adv_untargeted.npy\""
      ],
      "metadata": {
        "id": "S0N1Uv1Y_cLk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10d9a047-cdd8-4782-8a7d-4b83553af4ef"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "updating: x_adv_targeted.npy (deflated 10%)\n",
            "updating: x_adv_detect.npy (deflated 10%)\n",
            "updating: x_adv_jpeg.npy (deflated 10%)\n",
            "  adding: extraction.json (deflated 25%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with ZipFile(f\"{RESULTS_PATH}/results.zip\", 'r') as zip:\n",
        "    res = json.load(zip.open(\"extraction.json\"))\n",
        "    print_results(res)"
      ],
      "metadata": {
        "id": "VSPUajuP_zcd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "546966a4-6e26-4876-9eb5-ff86090d64a4"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "main character: 'Sherlock Holmes'\n",
            "greedy guess: '3\\n   '\n",
            "greedy numeric guess: '39731'\n",
            "exact guess: '35192'\n"
          ]
        }
      ]
    }
  ]
}