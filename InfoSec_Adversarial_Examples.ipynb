{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mseewer/infoseclab/blob/main/InfoSec_Adversarial_Examples.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Setup"
      ],
      "metadata": {
        "id": "6xO-XBsYMOiR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  # mount your google drive to get permanent storage for your results\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "\n",
        "  RESULTS_PATH = \"/content/drive/MyDrive/infoseclab_ML/results\"\n",
        "except ModuleNotFoundError:\n",
        "  RESULTS_PATH = \"results\"\n",
        "\n",
        "!mkdir -p {RESULTS_PATH}"
      ],
      "metadata": {
        "id": "BHkSCcGWfkls",
        "outputId": "26d5dddc-0988-494d-a219-7dcba997f7e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-43ad50857a60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0;31m# mount your google drive to get permanent storage for your results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mRESULTS_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/infoseclab_ML/results\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    102\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    122\u001b[0m       'TBE_EPHEM_CREDS_ADDR'] if ephemeral else _os.environ['TBE_CREDS_ADDR']\n\u001b[1;32m    123\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    125\u001b[0m         'request_auth', request={'authType': 'dfs_ephemeral'}, timeout_sec=None)\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    169\u001b[0m   request_id = send_request(\n\u001b[1;32m    170\u001b[0m       request_type, request, parent=parent, expect_reply=True)\n\u001b[0;32m--> 171\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    100\u001b[0m         reply.get('colab_msg_id') == message_id):\n\u001b[1;32m    101\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aCjvBq6rL0n1"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "# Lab files\n",
        "![ ! -d 'infoseclab' ] && git clone https://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@github.com/ethz-privsec/infoseclab.git\n",
        "%cd infoseclab\n",
        "!git pull https://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@github.com/ethz-privsec/infoseclab.git\n",
        "%cd ..\n",
        "if \"infoseclab\" not in sys.path:\n",
        "  sys.path.append(\"infoseclab\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Imports"
      ],
      "metadata": {
        "id": "LXC5q0RvNhhh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import infoseclab\n",
        "from infoseclab import defenses, attacks, ImageNet, EPSILON, utils, evaluation\n",
        "import os\n",
        "from zipfile import ZipFile\n",
        "\n",
        "device = \"cuda\""
      ],
      "metadata": {
        "id": "Uwi_QoU9Nguf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0.&nbsp;Untargeted attack on ResNet-50\n",
        "\n",
        "We will first run an *untargeted* attack, where the goal is to get the model to misclassify an input `(x, y)` as any incorrect class `y'`.\n",
        "\n",
        "Familiarize yourself with the provided code for a basic PGD attack. The attack is applied to clean images from ImageNet (`ImageNet.clean_images`) with their true labels (`ImageNet.labels`).\n",
        "\n"
      ],
      "metadata": {
        "id": "HV7wnlsjNyYb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resnet = defenses.ResNet(device)\n",
        "pgd = attacks.PGD(steps=20, epsilon=EPSILON, step_size=2.5 * EPSILON / 20, clf=resnet)\n",
        "x_adv = pgd.attack_all(ImageNet.clean_images, ImageNet.labels, verbose=False)\n",
        "\n",
        "utils.save_images(os.path.join(RESULTS_PATH, \"x_adv_untargeted.npy\"), x_adv)\n",
        "evaluation.eval_untargeted_pgd(os.path.join(RESULTS_PATH, \"x_adv_untargeted.npy\"), device)\n",
        "\n",
        "idx = 0\n",
        "input = torch.stack([x_adv[idx], ImageNet.clean_images[idx]]).to(device)\n",
        "logits = resnet.get_logits(input)\n",
        "utils.display(x_adv[idx], image_orig=ImageNet.clean_images[idx], logits=logits)"
      ],
      "metadata": {
        "id": "bEsJ9nz_N11y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1.&nbsp;Targeted attack on ResNet-50 \n",
        "\n",
        "The previous attack lowers the model's accuracy to 0%, but the mistakes are sometimes understandable (e.g., misclassifying a bowl of unripe \"rose hips\" as \"oranges\").\n",
        "\n",
        "You will now implement a *targeted* attack, where the goal is to get the model to misclassify an input `(x, y)` as a *specific* incorrect class `y'`.\n",
        "\n",
        "For this, you will amend the previous PGD attack to take as input a list of targets `ImageNet.targets` (one target class per input).\n"
      ],
      "metadata": {
        "id": "XW_kKeh1VZ5C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PGD_Targeted(attacks.PGD):\n",
        "    \"\"\"\n",
        "    A targeted PGD attack.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, steps, epsilon, step_size, clf):\n",
        "        \"\"\"\n",
        "        :param steps: the number of gradient steps to take\n",
        "        :param epsilon: the maximum perturbation allowed\n",
        "        :param step_size: the size of the gradient step\n",
        "        :param clf: the classifier to attack\n",
        "        \"\"\"\n",
        "        super().__init__(steps, epsilon, step_size, clf)\n",
        "\n",
        "    def attack_batch(self, x, y_targets, verbose=False):\n",
        "        \"\"\"\n",
        "        Attack a batch of images with targeted PGD.\n",
        "        :param x: the batch of images (torch tensors) to attack of size (batch_size, 3, 224, 224)\n",
        "        :param y: the target labels of size (batch_size,)\n",
        "        :param verbose: whether to print the progress of the attack\n",
        "        :return: the adversarial images of size (batch_size, 3, 224, 224)\n",
        "        \"\"\"\n",
        "        TODO MY LOCAL CHANGES\n",
        "        raise NotImplementedError()\n",
        "\n",
        "resnet = defenses.ResNet(device)\n",
        "pgd_t = PGD_Targeted(TODO)\n",
        "x_adv_t = pgd_t.attack_all(ImageNet.clean_images, ImageNet.targets, verbose=False)\n",
        "\n",
        "utils.save_images(os.path.join(RESULTS_PATH, \"x_adv_targeted.npy\"), x_adv_t)\n",
        "evaluation.eval_targeted_pgd(os.path.join(RESULTS_PATH, \"x_adv_targeted.npy\"), device)"
      ],
      "metadata": {
        "id": "U0lyNx1baNzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.&nbsp;Evading Detection\n",
        "\n",
        "It turns out that \"naive\" adversarial examples are very easy to *detect*.\n",
        "So one could build a defense that aims to detect when an input has been perturbed, to reject it and raise an alarm.\n",
        "\n",
        "Unfortunately, as we'll see such defenses are hard to make robust against an *adaptive* attacker that also optimizes over the detector."
      ],
      "metadata": {
        "id": "m9myeLu4a5xB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Our previous attack is easily detected\n",
        "evaluation.eval_detector_attack(os.path.join(RESULTS_PATH, \"x_adv_targeted.npy\"),\n",
        "                                detector_path=\"infoseclab/data/detector.pth\",\n",
        "                                device=device)"
      ],
      "metadata": {
        "id": "wEsVJ2M3a5Ku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You will now implement a targeted attack against the `ResNetDetector` defense.\n",
        "This defense takes the standard `ResNet` classifier from before, and adds an additional detector module.\n",
        "\n",
        "The defense can be used for classification, in which case it outputs an array of scores for each of the 1000 classes, for each input:\n",
        "\n",
        "```\n",
        "resnet_det = ResNetDetector(device)\n",
        "resnet_det.get_logits(x) -> [N, 1000]\n",
        "```\n",
        "\n",
        "To obtain a detector, we trained a *binary* classifier that takes in an input and outputs binary logits for the task of distinguishing clean images (class 0) from adversarially perturbed ones (class 1):\n",
        "\n",
        "```\n",
        "resnet_det = ResNetDetector(device)\n",
        "resnet_det.get_detection_logits(x) -> [N, 2]\n",
        "```\n",
        "\n",
        "*(the classifier and detector actually share most of their implementation.\n",
        "The original ResNet classifier is of the form `g(f(x))` where `f` is a <u>feature extractor</u> that maps inputs to feature vectors, and `g` is a <u>linear layer</u> that maps a feature vector to a vector of 1000 class scores.\n",
        "The detector takes as input the same feature vector `f(x)`, and applies a different linear layer `g_det` that maps the features to a vector of 2 class scores.\n",
        "See `infoseclab.defenses.defense_detector.ResNetDetector` for details).*\n",
        "\n",
        "Note: You are allowed to use the `ResNetDetector` module in your attack, but you are not allowed to modify it."
      ],
      "metadata": {
        "id": "XdM1aIZiyany"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PGD_Det(attacks.PGD):\n",
        "    \"\"\"\n",
        "    A targeted PGD attack that also tries to evade detection.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, steps, epsilon, step_size, clf):\n",
        "        \"\"\"\n",
        "        :param steps: the number of gradient steps to take\n",
        "        :param epsilon: the maximum perturbation allowed\n",
        "        :param step_size: the size of the gradient step\n",
        "        :param clf: the classifier to attack\n",
        "        \"\"\"\n",
        "        super().__init__(steps, epsilon, step_size, clf)\n",
        "\n",
        "    def attack_batch(self, x, y_targets, verbose=False):\n",
        "        \"\"\"\n",
        "        Attack a batch of images with targeted PGD while also evading detection.\n",
        "        :param x: the batch of images (torch tensors) to attack of size (batch_size, 3, 224, 224)\n",
        "        :param y_targets: the target labels of size (batch_size,)\n",
        "        :param verbose: whether to print the progress of the attack\n",
        "        :return: the adversarial images of size (batch_size, 3, 224, 224)\n",
        "        \"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "defense_det = defenses.ResNetDetector(device, detector_path=\"infoseclab/data/detector.pth\")\n",
        "pgd_det = PGD_Det(TODO)\n",
        "x_adv_det = pgd_det.attack_all(ImageNet.clean_images, ImageNet.targets, verbose=False)\n",
        "\n",
        "utils.save_images(os.path.join(RESULTS_PATH, \"x_adv_detect.npy\"), x_adv_det)\n",
        "evaluation.eval_detector_attack(os.path.join(RESULTS_PATH, \"x_adv_detect.npy\"), device=device)"
      ],
      "metadata": {
        "id": "W7V6HnP5b4LR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3.&nbsp; Preprocessing Defenses (pick one!)\n",
        "\n",
        "We are now going to look at two simple defenses against adversarial examples that aim to resist noise by *pre-processing* the input before classifying it.\n",
        "\n",
        "**Your goal is to break <u>ONE</u> of these two defenses (you can choose which one you prefer)!**\n",
        "\n",
        "<ul>\n",
        "  <li> 3.1. JPEG Compression </li>\n",
        "  <li> 3.2. Random cropping and noising </li>\n",
        "<ul>"
      ],
      "metadata": {
        "id": "EdorMrPt-Vpf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3.1&nbsp; JPEG Compression\n",
        "\n",
        "A natural defense idea is to try and remove the noise from adversarial images. A simple way of trying to do that is to compress images before classifying them, e.g., with JPEG.\n",
        "\n",
        "The `ResNetJPEG` defense implements this."
      ],
      "metadata": {
        "id": "3y_q-ox3c_qb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Neither of our attacks survives JPEG compression\n",
        "evaluation.eval_jpeg_attack(os.path.join(RESULTS_PATH, \"x_adv_targeted.npy\"), device)\n",
        "evaluation.eval_jpeg_attack(os.path.join(RESULTS_PATH, \"x_adv_detect.npy\"), device)"
      ],
      "metadata": {
        "id": "jyDfOTPtdE_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your goal is to create a targeted PGD attack that will defeat JPEG compression.\n",
        "In this part, you **don't** need to ensure that the attack stays undetected.\n",
        "\n",
        "**For this attack (<u>and only for this attack</u>), you are allowed to import additional modules from GitHub if you want. However, you are not allowed to use any external modules that implement adversarial examples attacks (e.g., cleverhans, foolbox, etc.)</br>\n",
        "If you're unsure if a module is ok to use, please ask!**\n",
        "\n",
        "External modules can be added in different ways:\n",
        "*   using PIP: `!pip install XXX`\n",
        "*   by cloning a github repository locally (replace the git URL and RESPOSITORY_NAME accordingly):\n",
        "\n",
        "```\n",
        "!git clone {URL}\n",
        "if {RESPOSITORY_NAME} not in sys.path:\n",
        "  sys.path.append(\"{RESPOSITORY_NAME}\")\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "lTr4mUvc1UAK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PGD_JPEG(attacks.PGD):\n",
        "    \"\"\"\n",
        "    A targeted PGD attack that tries to resist JPEG compression.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, steps, epsilon, step_size, clf):\n",
        "        \"\"\"\n",
        "        :param steps: the number of gradient steps to take\n",
        "        :param epsilon: the maximum perturbation allowed\n",
        "        :param step_size: the size of the gradient step\n",
        "        :param clf: the classifier to attack\n",
        "        \"\"\"\n",
        "        super().__init__(steps, epsilon, step_size, clf)\n",
        "\n",
        "        \n",
        "    def attack_batch(self, x, y_targets, verbose=False):\n",
        "        \"\"\"\n",
        "        Attack a batch of images with targeted PGD while also resisting JPEG compression.\n",
        "        :param x: the batch of images (torch tensors) to attack of size (batch_size, 3, 224, 224)\n",
        "        :param y_targets: the target labels of size (batch_size,)\n",
        "        :param verbose: whether to print the progress of the attack\n",
        "        :return: the adversarial images of size (batch_size, 3, 224, 224)\n",
        "        \"\"\"\n",
        "\n",
        "        raise NotImplementedError()\n",
        "\n",
        "defense = defenses.ResNetJPEG(device)\n",
        "pgd_jpeg = PGD_JPEG(TODO)\n",
        "x_adv_jpeg = pgd_jpeg.attack_all(ImageNet.clean_images, ImageNet.targets, verbose=False)\n",
        "\n",
        "utils.save_images(os.path.join(RESULTS_PATH, \"x_adv_jpeg.npy\"), x_adv_jpeg)\n",
        "evaluation.eval_jpeg_attack(os.path.join(RESULTS_PATH, \"x_adv_jpeg.npy\"), device)"
      ],
      "metadata": {
        "id": "GlbbYrsIgnsc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3.2&nbsp; Randomized cropping and noising\n",
        "\n",
        "Another natural defense idea is to try and *randomize* the model's behavior to make it harder to create adversarial examples.\n",
        "\n",
        "The ResNetRandom defense implements this, by randomly cropping and noising input images before classifying them."
      ],
      "metadata": {
        "id": "ag6TwSfy_Y8Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Neither of our attacks survive random preprocessing\n",
        "evaluation.eval_random_attack(os.path.join(RESULTS_PATH, \"x_adv_targeted.npy\"), device)\n",
        "evaluation.eval_random_attack(os.path.join(RESULTS_PATH, \"x_adv_detect.npy\"), device)"
      ],
      "metadata": {
        "id": "dlXyTEGO_8bl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your goal is to create a targeted PGD attack that will defeat randomized pre-processing.\n",
        "In this part, you **don't** need to ensure that the attack stays undetected.\n",
        "\n",
        "**Note that since this defense is randomized, the evaluation results might vary slightly from one run to the next. To make sure that your attack passes our final evaluation, try to create an attack that has a few % of slack compared to the evaluation targets (e.g., if we target an adversarial accuracy below 5%, aim to ensure that your attack reaches ~3% or lower)**"
      ],
      "metadata": {
        "id": "kW_63qhMAF4h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PGD_Random(attacks.PGD):\n",
        "    \"\"\"\n",
        "    A PGD attack that also tries to resist random preprocessing.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, steps, epsilon, step_size, clf):\n",
        "        super().__init__(steps, epsilon, step_size, clf)\n",
        "        \n",
        "    def attack_batch(self, x, y_targets, verbose=False):\n",
        "        \"\"\"\n",
        "        Attack a batch of images with targeted PGD while also evading random preprocessing.\n",
        "        :param x: the batch of images (torch tensors) to attack of size (batch_size, 3, 224, 224)\n",
        "        :param y_targets: the target labels of size (batch_size,)\n",
        "        :param verbose: whether to print the progress of the attack\n",
        "        :return: the adversarial images of size (batch_size, 3, 224, 224)\n",
        "        \"\"\"\n",
        "\n",
        "        raise NotImplementedError()\n",
        "\n",
        "\n",
        "defense = defenses.ResNetRandom(device)\n",
        "pgd_random = PGD_Random(steps=10, epsilon=EPSILON, step_size=1, clf=defense)\n",
        "x_adv_random = pgd_random.attack_all(ImageNet.clean_images, ImageNet.targets, verbose=False)\n",
        "\n",
        "utils.save_images(os.path.join(RESULTS_PATH, \"x_adv_random.npy\"), x_adv_random)\n",
        "evaluation.eval_random_attack(os.path.join(RESULTS_PATH, \"x_adv_random.npy\"), device)"
      ],
      "metadata": {
        "id": "sH64CeqCAzFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create submission file (**upload `results.zip` to moodle**)"
      ],
      "metadata": {
        "id": "9_Cmw-Qj8mLe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -j -r \"{RESULTS_PATH}/results.zip\" {RESULTS_PATH} --exclude \"*x_adv_untargeted.npy\""
      ],
      "metadata": {
        "id": "197yEKu1J_-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from infoseclab.evaluation import eval_targeted_pgd, eval_detector_attack, eval_jpeg_attack, eval_random_attack\n",
        "with ZipFile(f\"{RESULTS_PATH}/results.zip\", 'r') as zip:\n",
        "    eval_targeted_pgd(path=zip.open(\"x_adv_targeted.npy\"), device=device)\n",
        "    eval_detector_attack(path=zip.open(\"x_adv_detect.npy\"), device=device)\n",
        "    eval_jpeg_attack(path=zip.open(\"x_adv_jpeg.npy\"), device=device)\n",
        "    eval_random_attack(path=zip.open(\"x_adv_random.npy\"), device=device)"
      ],
      "metadata": {
        "id": "yW3j3t9y9ZVO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}